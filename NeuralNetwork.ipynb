{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9OURlIZ6LiFxjbR3ZEuL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datapirate09/Neural-Network-Builder/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "po-GlxcIpAYb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xqaLhvrsXytC"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, activation='sigmoid'):\n",
        "    #layers is a list of elements where each element shows no of neurons in that layer\n",
        "    self.no_of_layers = len(layers)\n",
        "    self.sizes = layers\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
        "    self.biases = [np.random.randn(y,1) for y in layers[1:]]\n",
        "    self.activation = activation\n",
        "\n",
        "\n",
        "  def get_shape_of_weights(self):\n",
        "    for i,layer in enumerate(self.weights):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def get_shape_of_bias(self):\n",
        "    for i,layer in enumerate(self.biases):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "  def sigmoid_derivative(self, z):\n",
        "    return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "\n",
        "  def activation_function(self,activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid(activation_input)\n",
        "\n",
        "\n",
        "  def activation_function_derivative(self, activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid_derivative(activation_input)\n",
        "\n",
        "\n",
        "  def forward_propagation(self, a):\n",
        "    for i in range(self.no_of_layers-1):\n",
        "      a = self.activation_function(np.dot(self.weights[i],a) + self.biases[i])\n",
        "    return a\n",
        "\n",
        "\n",
        "  def update_weights_and_biases(self, input_data, batch_size=10, learning_rate=3.0):\n",
        "    random.shuffle(input_data)\n",
        "    mini_batches = [input_data[k:k+batch_size] for k in range(0, len(input_data), batch_size)]\n",
        "    for batch in mini_batches:\n",
        "        self.update_mini_batch(batch, learning_rate)\n",
        "\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, learning_rate=3.0):\n",
        "    b_diff = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_diff = [np.zeros(w.shape) for w in self.weights]\n",
        "    for data_item in mini_batch:\n",
        "        x, y = data_item  # training input and label\n",
        "        activations, z_vector = self.get_activations(x)\n",
        "        b_error, w_error = self.back_propagation(activations, z_vector, y)\n",
        "        b_diff = [bd + be for bd, be in zip(b_diff, b_error)]\n",
        "        w_diff = [wd + we for wd, we in zip(w_diff, w_error)]\n",
        "\n",
        "    self.weights = [w - (learning_rate / len(mini_batch)) * nw\n",
        "                    for w, nw in zip(self.weights, w_diff)]\n",
        "    self.biases = [b - (learning_rate / len(mini_batch)) * nb\n",
        "                   for b, nb in zip(self.biases, b_diff)]\n",
        "\n",
        "\n",
        "  def get_activations(self, input_data):\n",
        "    activation = input_data.reshape(-1, 1)  # Ensure it's a column vector\n",
        "    activations = [activation]\n",
        "    z_vector = []\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        z_vector.append(z)\n",
        "        activation = self.activation_function(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "    return activations, z_vector\n",
        "\n",
        "\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "    return (output_activations-y)\n",
        "\n",
        "\n",
        "  def back_propagation(self, activations, z_vector, y):\n",
        "    b_error = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_error = [np.zeros(w.shape) for w in self.weights]\n",
        "    delta = self.cost_derivative(activations[-1], y) * self.activation_function_derivative(z_vector[-1])\n",
        "    b_error[-1] = delta\n",
        "    w_error[-1] = np.dot(delta, activations[-2].transpose())\n",
        "    for l in range(2, self.no_of_layers):\n",
        "        z = z_vector[-l]\n",
        "        sp = self.activation_function_derivative(z)\n",
        "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "        b_error[-l] = delta\n",
        "        w_error[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    return (b_error, w_error)\n",
        "\n",
        "\n",
        "  def fit(self, training_data, training_labels, validation_data=None, validation_labels=None, epochs=20, batch_size = 10, learning_rate = 3.0, callbacks=None):\n",
        "    n = len(training_data)\n",
        "    training_data_combined = list(zip(training_data, training_labels))\n",
        "    if validation_data != None and validation_labels != None:\n",
        "      validation_data_combined = list(zip(validation_data, validation_labels))\n",
        "    early_stopping = None\n",
        "    model_check_point = None\n",
        "    reduce_lr_on_plateau = None\n",
        "    if callbacks is None:\n",
        "      callbacks = []\n",
        "    for cb in callbacks:\n",
        "      if isinstance(cb, EarlyStopping):\n",
        "        early_stopping = cb\n",
        "      if isinstance(cb, ModelCheckPoint):\n",
        "        model_check_point = cb\n",
        "      if isinstance(cb, ReduceLROnPlateau):\n",
        "        reduce_lr_on_plateau = cb\n",
        "    for iteration in range(epochs):\n",
        "      start_time = time.time()\n",
        "      self.update_weights_and_biases(training_data_combined, batch_size, learning_rate)\n",
        "      accuracy, loss = self.evaluate(training_data_combined)\n",
        "      logs = {\"accuracy\": accuracy, \"loss\": loss}\n",
        "      duration = time.time() - start_time\n",
        "      if validation_data != None and validation_labels != None:\n",
        "        val_accuracy, val_loss = self.evaluate(validation_data_combined)\n",
        "        logs[\"val_accuracy\"] = val_accuracy\n",
        "        logs[\"val_loss\"] = val_loss\n",
        "        print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f} - val_accuracy: {val_accuracy:.4f} - val_loss: {val_loss:.4f}\")\n",
        "      else:\n",
        "        print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f}\")\n",
        "      if early_stopping:\n",
        "        early_stopping.on_epoch_end(iteration, logs)\n",
        "        if early_stopping.stop_training:\n",
        "            break\n",
        "      if model_check_point:\n",
        "        model_check_point.on_epoch_end(iteration, logs)\n",
        "        if model_check_point.show_check_point:\n",
        "          best_epoch_details = model_check_point.get_best_epoch_details()\n",
        "          print(f\"accuracy: {best_epoch_details['accuracy']:.4f} - loss: {best_epoch_details['loss']:.4f} - val_accuracy: {best_epoch_details['val_accuracy']:.4f} - val_loss: {best_epoch_details['val_loss']:.4f}\")\n",
        "      if reduce_lr_on_plateau:\n",
        "        factor = reduce_lr_on_plateau.on_epoch_end(iteration, logs)\n",
        "        if factor is not None:\n",
        "          new_lr = max(learning_rate * factor, reduce_lr_on_plateau.min_lr)\n",
        "          print(f\"Reducing learning rate from {learning_rate:.5f} to {new_lr:.5f}\")\n",
        "          learning_rate = new_lr\n",
        "\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for x, y in data:\n",
        "        output = self.forward_propagation(x)\n",
        "        predicted = np.argmax(output)\n",
        "        actual = np.argmax(y)\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "        total_loss += np.sum((output - y) ** 2)\n",
        "    accuracy = correct / len(data)\n",
        "    avg_loss = total_loss / len(data)\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "\n",
        "  def predict(self, data):\n",
        "    predictions = []\n",
        "    for item in data:\n",
        "      predictions.append(self.forward_propagation(item))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "  def __init__(self, monitor='val_loss', patience=0):\n",
        "    self.patience = patience\n",
        "    self.current_count = 0\n",
        "    self.monitor = monitor\n",
        "    self.best_value = None\n",
        "    self.stop_training = False\n",
        "\n",
        "  def on_epoch_end(self, epoch, epoch_info):\n",
        "    current = epoch_info.get(self.monitor)\n",
        "    if current is None:\n",
        "      return\n",
        "    if self.best_value == None or current < self.best_value:\n",
        "      self.best_value = current\n",
        "      self.current_count = 0\n",
        "    else:\n",
        "      self.current_count += 1\n",
        "      if self.current_count > self.patience:\n",
        "        self.stop_training = True\n",
        "\n",
        "\n",
        "class ModelCheckPoint:\n",
        "  def __init__(self, monitor='val_loss'):\n",
        "    self.monitor = monitor\n",
        "    self.show_check_point = False\n",
        "    self.previous_best = None\n",
        "    self.improved_epoch_details = []\n",
        "  def on_epoch_end(self, epoch, epoch_info):\n",
        "    current = epoch_info.get(self.monitor)\n",
        "    if self.previous_best == None or current < self.previous_best:\n",
        "      self.previous_best = current\n",
        "      self.store_epoch_details(epoch_info)\n",
        "      self.show_check_point = True\n",
        "    else:\n",
        "      self.show_check_point = False\n",
        "  def store_epoch_details(self, epoch_info):\n",
        "    self.improved_epoch_details.append(epoch_info)\n",
        "  def get_best_epoch_details(self):\n",
        "    return self.improved_epoch_details[-1]\n",
        "\n",
        "\n",
        "class ReduceLROnPlateau:\n",
        "  def __init__(self, monitor='val_loss', factor=0.1, patience=10, mode='min', min_lr=0.0):\n",
        "    self.monitor = monitor\n",
        "    self.factor = factor\n",
        "    self.patience = patience\n",
        "    self.mode = mode\n",
        "    self.min_lr = min_lr\n",
        "    self.current_count = 0\n",
        "    self.optimal_val = None\n",
        "\n",
        "  def on_epoch_end(self, epoch, epoch_info):\n",
        "    current = epoch_info.get(self.monitor)\n",
        "    if current is None:\n",
        "      return None\n",
        "    if self.optimal_val == None or self.check_optimal_condition(current):\n",
        "      self.optimal_val = current\n",
        "      self.current_count = 0\n",
        "      return None\n",
        "    else:\n",
        "      self.current_count += 1\n",
        "      if self.current_count > self.patience:\n",
        "        return self.factor\n",
        "\n",
        "  def check_optimal_condition(self, value):\n",
        "    if self.mode == 'min':\n",
        "      if value < self.optimal_val:\n",
        "        return True\n",
        "    elif self.mode == 'max':\n",
        "      if value > self.optimal_val:\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pw7FOzTq5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz -O mnist.pkl.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceijdXIYJ_i",
        "outputId": "394bc8c5-e331-4668-8024-0ed099b93d72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-08 14:39:54--  https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz [following]\n",
            "--2025-04-08 14:39:54--  https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17051982 (16M) [application/octet-stream]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "mnist.pkl.gz        100%[===================>]  16.26M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-08 14:39:55 (131 MB/s) - ‘mnist.pkl.gz’ saved [17051982/17051982]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBdZbdYnIaYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "file_path = \"mnist.pkl.gz\"\n",
        "\n",
        "with gzip.open(file_path, 'rb') as f:\n",
        "    mnist_data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "training_data, validation_data, test_data = mnist_data\n",
        "print(training_data[0].shape)\n",
        "print(training_data[1].shape)\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "X_train = [x.reshape(-1, 1) for x in training_data[0]]\n",
        "y_train = [vectorized_label(y) for y in training_data[1]]\n",
        "\n",
        "X_val = [x.reshape(-1, 1) for x in validation_data[0]]\n",
        "y_val = [vectorized_label(y) for y in validation_data[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDiZDPkY9z8",
        "outputId": "f366da78-f342-40c4-c53d-512bdf8ca00e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork([784,30,10])"
      ],
      "metadata": {
        "id": "5tmOoXc8IeNX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_check_point = ModelCheckPoint(monitor='val_loss')\n",
        "model.fit(X_train, y_train, X_val, y_val, callbacks=[early_stopping,model_check_point])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "gQGxRFfFIkHu",
        "outputId": "1044ae05-01be-4c92-e4e0-d179a77db04f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9093 - loss: 0.1502 - val_accuracy: 0.9131 - val_loss: 0.1409\n",
            "accuracy: 0.9093 - loss: 0.1502 - val_accuracy: 0.9131 - val_loss: 0.1409\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9233 - loss: 0.1296 - val_accuracy: 0.9278 - val_loss: 0.1230\n",
            "accuracy: 0.9233 - loss: 0.1296 - val_accuracy: 0.9278 - val_loss: 0.1230\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-34e77ae4a54b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_check_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_check_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-ac93d67f74a1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, training_data, training_labels, validation_data, validation_labels, epochs, batch_size, learning_rate, callbacks)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights_and_biases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m       \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ac93d67f74a1>\u001b[0m in \u001b[0;36mupdate_weights_and_biases\u001b[0;34m(self, input_data, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ac93d67f74a1>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_item\u001b[0m  \u001b[0;31m# training input and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mb_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mb_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_diff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mw_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_diff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ac93d67f74a1>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(self, activations, z_vector, y)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mb_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mw_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = [x.reshape(-1, 1) for x in test_data[0]]\n",
        "y_test = [vectorized_label(y) for y in test_data[1]]\n",
        "testing_data_combined = list(zip(X_test, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions[0])\n"
      ],
      "metadata": {
        "id": "lVA16rKLPukc",
        "outputId": "abb22a09-2930-4446-cbf3-365ccc6f24e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9429\n",
            "0.09415478334754462\n",
            "[[1.71364994e-06]\n",
            " [2.36814271e-08]\n",
            " [6.89708166e-07]\n",
            " [7.66631390e-06]\n",
            " [6.93306109e-08]\n",
            " [1.21063479e-08]\n",
            " [1.71949977e-12]\n",
            " [9.99832671e-01]\n",
            " [5.54620956e-10]\n",
            " [1.13528195e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "(X_train_tf, y_train_tf), (X_test_tf, y_test_tf) = fashion_mnist.load_data()\n",
        "\n",
        "X_train_tf = [x.reshape(-1, 1) / 255.0 for x in X_train_tf]\n",
        "y_train_tf = [vectorized_label(y) for y in y_train_tf]\n",
        "\n",
        "X_test_tf = [x.reshape(-1, 1) / 255.0 for x in X_test_tf]\n",
        "y_test = [vectorized_label(y) for y in y_test_tf]\n",
        "\n",
        "model = NeuralNetwork([784, 64, 10])\n",
        "model.fit(X_train_tf, y_train_tf, epochs=10, batch_size=32, learning_rate=3.0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o6JwzlVgMUMN",
        "outputId": "4770eea8-58d9-4a16-c304-687c8ac97ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7493 - loss: 0.3547\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7744 - loss: 0.3132\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7856 - loss: 0.3009\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7882 - loss: 0.2916\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7971 - loss: 0.2780\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7972 - loss: 0.2803\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8072 - loss: 0.2646\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8101 - loss: 0.2587\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8092 - loss: 0.2620\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8123 - loss: 0.2603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data_combined = list(zip(X_test_tf, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)"
      ],
      "metadata": {
        "id": "-rCiJ3oHSHSd",
        "outputId": "721d3180-c9c9-4fa4-a52f-7d74f645ead2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7957\n",
            "0.28854674926965623\n"
          ]
        }
      ]
    }
  ]
}