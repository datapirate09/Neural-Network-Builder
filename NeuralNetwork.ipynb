{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8Lj6qIDQkrK4volWHvoW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datapirate09/Neural-Network-Builder/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "po-GlxcIpAYb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xqaLhvrsXytC"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, activation='sigmoid'):\n",
        "    #layers is a list of elements where each element shows no of neurons in that layer\n",
        "    self.no_of_layers = len(layers)\n",
        "    self.sizes = layers\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
        "    self.biases = [np.random.randn(y,1) for y in layers[1:]]\n",
        "    self.activation = activation\n",
        "\n",
        "\n",
        "  def get_shape_of_weights(self):\n",
        "    for i,layer in enumerate(self.weights):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def get_shape_of_bias(self):\n",
        "    for i,layer in enumerate(self.biases):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "  def sigmoid_derivative(self, z):\n",
        "    return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "\n",
        "  def activation_function(self,activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid(activation_input)\n",
        "\n",
        "\n",
        "  def activation_function_derivative(self, activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid_derivative(activation_input)\n",
        "\n",
        "\n",
        "  def forward_propagation(self, a):\n",
        "    for i in range(self.no_of_layers-1):\n",
        "      a = self.activation_function(np.dot(self.weights[i],a) + self.biases[i])\n",
        "    return a\n",
        "\n",
        "\n",
        "  def update_weights_and_biases(self, input_data, batch_size=10, learning_rate=3.0):\n",
        "    random.shuffle(input_data)\n",
        "    mini_batches = [input_data[k:k+batch_size] for k in range(0, len(input_data), batch_size)]\n",
        "    for batch in mini_batches:\n",
        "        self.update_mini_batch(batch, learning_rate)\n",
        "\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, learning_rate=3.0):\n",
        "    b_diff = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_diff = [np.zeros(w.shape) for w in self.weights]\n",
        "    for data_item in mini_batch:\n",
        "        x, y = data_item  # training input and label\n",
        "        activations, z_vector = self.get_activations(x)\n",
        "        b_error, w_error = self.back_propagation(activations, z_vector, y)\n",
        "        b_diff = [bd + be for bd, be in zip(b_diff, b_error)]\n",
        "        w_diff = [wd + we for wd, we in zip(w_diff, w_error)]\n",
        "\n",
        "    self.weights = [w - (learning_rate / len(mini_batch)) * nw\n",
        "                    for w, nw in zip(self.weights, w_diff)]\n",
        "    self.biases = [b - (learning_rate / len(mini_batch)) * nb\n",
        "                   for b, nb in zip(self.biases, b_diff)]\n",
        "\n",
        "\n",
        "  def get_activations(self, input_data):\n",
        "    activation = input_data.reshape(-1, 1)  # Ensure it's a column vector\n",
        "    activations = [activation]\n",
        "    z_vector = []\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        z_vector.append(z)\n",
        "        activation = self.activation_function(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "    return activations, z_vector\n",
        "\n",
        "\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "\n",
        "  def back_propagation(self, activations, z_vector, y):\n",
        "    b_error = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_error = [np.zeros(w.shape) for w in self.weights]\n",
        "    delta = self.cost_derivative(activations[-1], y) * self.activation_function_derivative(z_vector[-1])\n",
        "    b_error[-1] = delta\n",
        "    w_error[-1] = np.dot(delta, activations[-2].transpose())\n",
        "    for l in range(2, self.no_of_layers):\n",
        "        z = z_vector[-l]\n",
        "        sp = self.activation_function_derivative(z)\n",
        "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "        b_error[-l] = delta\n",
        "        w_error[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    return (b_error, w_error)\n",
        "\n",
        "\n",
        "  def fit(self, training_data, training_labels, validation_data=None, validation_labels=None, epochs=20, batch_size = 10, learning_rate = 3.0, callbacks=None):\n",
        "    n = len(training_data)\n",
        "    training_data_combined = list(zip(training_data, training_labels))\n",
        "    if validation_data != None and validation_labels != None:\n",
        "      validation_data_combined = list(zip(validation_data, validation_labels))\n",
        "    early_stopping = None\n",
        "    for cb in callbacks:\n",
        "      if isinstance(cb, EarlyStopping):\n",
        "        early_stopping = cb\n",
        "    for iteration in range(epochs):\n",
        "      start_time = time.time()\n",
        "      self.update_weights_and_biases(training_data_combined, batch_size, learning_rate)\n",
        "      accuracy, loss = self.evaluate(training_data_combined)\n",
        "      logs = {\"accuracy\": accuracy, \"loss\": loss}\n",
        "      duration = time.time() - start_time\n",
        "      if validation_data != None and validation_labels != None:\n",
        "        val_accuracy, val_loss = self.evaluate(validation_data_combined)\n",
        "        logs[\"val_accuracy\"] = val_accuracy\n",
        "        logs[\"val_loss\"] = val_loss\n",
        "        print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f} - val_accuracy: {val_accuracy:.4f} - val_loss: {val_loss:.4f}\")\n",
        "      else:\n",
        "        print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f}\")\n",
        "      if early_stopping:\n",
        "        early_stopping.on_epoch_end(iteration, logs)\n",
        "        if early_stopping.stop_training:\n",
        "            break\n",
        "\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for x, y in data:\n",
        "        output = self.forward_propagation(x)\n",
        "        predicted = np.argmax(output)\n",
        "        actual = np.argmax(y)\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "        total_loss += np.sum((output - y) ** 2)\n",
        "    accuracy = correct / len(data)\n",
        "    avg_loss = total_loss / len(data)\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "\n",
        "  def predict(self, data):\n",
        "    predictions = []\n",
        "    for item in data:\n",
        "      predictions.append(self.forward_propagation(item))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "  def __init__(self, monitor='val_loss', patience=0):\n",
        "    self.patience = patience\n",
        "    self.current_count = 0\n",
        "    self.monitor = monitor\n",
        "    self.best_value = None\n",
        "    self.stop_training = False\n",
        "\n",
        "  def on_epoch_end(self, epoch, epoch_info):\n",
        "    current = epoch_info.get(self.monitor)\n",
        "    if current is None:\n",
        "      return\n",
        "    if self.best_value == None or current < self.best_value:\n",
        "      self.best_value = current\n",
        "      self.current_count = 0\n",
        "    else:\n",
        "      self.current_count += 1\n",
        "      if self.current_count > self.patience:\n",
        "        self.stop_training = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pw7FOzTq5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz -O mnist.pkl.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceijdXIYJ_i",
        "outputId": "40632975-9df6-4f36-87c8-9b46609b5563"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-07 15:48:30--  https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz [following]\n",
            "--2025-04-07 15:48:30--  https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17051982 (16M) [application/octet-stream]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "mnist.pkl.gz        100%[===================>]  16.26M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-07 15:48:30 (138 MB/s) - ‘mnist.pkl.gz’ saved [17051982/17051982]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBdZbdYnIaYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "file_path = \"mnist.pkl.gz\"\n",
        "\n",
        "with gzip.open(file_path, 'rb') as f:\n",
        "    mnist_data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "training_data, validation_data, test_data = mnist_data\n",
        "print(training_data[0].shape)\n",
        "print(training_data[1].shape)\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "X_train = [x.reshape(-1, 1) for x in training_data[0]]\n",
        "y_train = [vectorized_label(y) for y in training_data[1]]\n",
        "\n",
        "X_val = [x.reshape(-1, 1) for x in validation_data[0]]\n",
        "y_val = [vectorized_label(y) for y in validation_data[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDiZDPkY9z8",
        "outputId": "f8d242a9-7c73-4f67-a238-17c4132b039d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork([784,30,10])"
      ],
      "metadata": {
        "id": "5tmOoXc8IeNX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model.fit(X_train, y_train, X_val, y_val, callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQGxRFfFIkHu",
        "outputId": "1130b2f2-a0fa-447b-bdb5-27ad31eace58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9055 - loss: 0.1546 - val_accuracy: 0.9136 - val_loss: 0.1445\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9252 - loss: 0.1226 - val_accuracy: 0.9315 - val_loss: 0.1167\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9322 - loss: 0.1141 - val_accuracy: 0.9338 - val_loss: 0.1097\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9417 - loss: 0.0980 - val_accuracy: 0.9385 - val_loss: 0.1010\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9387 - loss: 0.1031 - val_accuracy: 0.9365 - val_loss: 0.1061\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9447 - loss: 0.0928 - val_accuracy: 0.9406 - val_loss: 0.1012\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9498 - loss: 0.0851 - val_accuracy: 0.9437 - val_loss: 0.0944\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9528 - loss: 0.0791 - val_accuracy: 0.9466 - val_loss: 0.0883\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9533 - loss: 0.0791 - val_accuracy: 0.9476 - val_loss: 0.0908\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9549 - loss: 0.0765 - val_accuracy: 0.9465 - val_loss: 0.0900\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9575 - loss: 0.0721 - val_accuracy: 0.9463 - val_loss: 0.0885\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9581 - loss: 0.0711 - val_accuracy: 0.9477 - val_loss: 0.0880\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9571 - loss: 0.0730 - val_accuracy: 0.9463 - val_loss: 0.0901\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9601 - loss: 0.0677 - val_accuracy: 0.9467 - val_loss: 0.0874\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9616 - loss: 0.0650 - val_accuracy: 0.9487 - val_loss: 0.0858\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9608 - loss: 0.0678 - val_accuracy: 0.9472 - val_loss: 0.0896\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9620 - loss: 0.0644 - val_accuracy: 0.9498 - val_loss: 0.0847\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9598 - loss: 0.0690 - val_accuracy: 0.9471 - val_loss: 0.0893\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9656 - loss: 0.0582 - val_accuracy: 0.9517 - val_loss: 0.0823\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9611 - loss: 0.0675 - val_accuracy: 0.9460 - val_loss: 0.0906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = [x.reshape(-1, 1) for x in test_data[0]]\n",
        "y_test = [vectorized_label(y) for y in test_data[1]]\n",
        "testing_data_combined = list(zip(X_test, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions[0])\n"
      ],
      "metadata": {
        "id": "lVA16rKLPukc",
        "outputId": "abb22a09-2930-4446-cbf3-365ccc6f24e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9429\n",
            "0.09415478334754462\n",
            "[[1.71364994e-06]\n",
            " [2.36814271e-08]\n",
            " [6.89708166e-07]\n",
            " [7.66631390e-06]\n",
            " [6.93306109e-08]\n",
            " [1.21063479e-08]\n",
            " [1.71949977e-12]\n",
            " [9.99832671e-01]\n",
            " [5.54620956e-10]\n",
            " [1.13528195e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "(X_train_tf, y_train_tf), (X_test_tf, y_test_tf) = fashion_mnist.load_data()\n",
        "\n",
        "X_train_tf = [x.reshape(-1, 1) / 255.0 for x in X_train_tf]\n",
        "y_train_tf = [vectorized_label(y) for y in y_train_tf]\n",
        "\n",
        "X_test_tf = [x.reshape(-1, 1) / 255.0 for x in X_test_tf]\n",
        "y_test = [vectorized_label(y) for y in y_test_tf]\n",
        "\n",
        "model = NeuralNetwork([784, 64, 10])\n",
        "model.fit(X_train_tf, y_train_tf, epochs=10, batch_size=32, learning_rate=3.0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o6JwzlVgMUMN",
        "outputId": "4770eea8-58d9-4a16-c304-687c8ac97ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7493 - loss: 0.3547\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7744 - loss: 0.3132\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7856 - loss: 0.3009\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7882 - loss: 0.2916\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7971 - loss: 0.2780\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7972 - loss: 0.2803\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8072 - loss: 0.2646\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8101 - loss: 0.2587\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8092 - loss: 0.2620\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8123 - loss: 0.2603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data_combined = list(zip(X_test_tf, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)"
      ],
      "metadata": {
        "id": "-rCiJ3oHSHSd",
        "outputId": "721d3180-c9c9-4fa4-a52f-7d74f645ead2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7957\n",
            "0.28854674926965623\n"
          ]
        }
      ]
    }
  ]
}