{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqWaiYyn0kCRMeLoXqNfA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datapirate09/Neural-Network-Builder/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "po-GlxcIpAYb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xqaLhvrsXytC"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, activation='sigmoid'):\n",
        "    #layers is a list of elements where each element shows no of neurons in that layer\n",
        "    self.no_of_layers = len(layers)\n",
        "    self.sizes = layers\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
        "    self.biases = [np.random.randn(y,1) for y in layers[1:]]\n",
        "    self.activation = activation\n",
        "\n",
        "\n",
        "  def get_shape_of_weights(self):\n",
        "    for i,layer in enumerate(self.weights):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def get_shape_of_bias(self):\n",
        "    for i,layer in enumerate(self.biases):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "  def sigmoid_derivative(self, z):\n",
        "    return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "\n",
        "  def activation_function(self,activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid(activation_input)\n",
        "\n",
        "\n",
        "  def activation_function_derivative(self, activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid_derivative(activation_input)\n",
        "\n",
        "\n",
        "  def forward_propagation(self, a):\n",
        "    for i in range(self.no_of_layers-1):\n",
        "      a = self.activation_function(np.dot(self.weights[i],a) + self.biases[i])\n",
        "    return a\n",
        "\n",
        "\n",
        "  def update_weights_and_biases(self, input_data, batch_size=10, learning_rate=3.0):\n",
        "    random.shuffle(input_data)\n",
        "    mini_batches = [input_data[k:k+batch_size] for k in range(0, len(input_data), batch_size)]\n",
        "    for batch in mini_batches:\n",
        "        self.update_mini_batch(batch, learning_rate)\n",
        "\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, learning_rate=3.0):\n",
        "    b_diff = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_diff = [np.zeros(w.shape) for w in self.weights]\n",
        "    for data_item in mini_batch:\n",
        "        x, y = data_item  # training input and label\n",
        "        activations, z_vector = self.get_activations(x)\n",
        "        b_error, w_error = self.back_propagation(activations, z_vector, y)\n",
        "        b_diff = [bd + be for bd, be in zip(b_diff, b_error)]\n",
        "        w_diff = [wd + we for wd, we in zip(w_diff, w_error)]\n",
        "\n",
        "    self.weights = [w - (learning_rate / len(mini_batch)) * nw\n",
        "                    for w, nw in zip(self.weights, w_diff)]\n",
        "    self.biases = [b - (learning_rate / len(mini_batch)) * nb\n",
        "                   for b, nb in zip(self.biases, b_diff)]\n",
        "\n",
        "\n",
        "  def get_activations(self, input_data):\n",
        "    activation = input_data.reshape(-1, 1)  # Ensure it's a column vector\n",
        "    activations = [activation]\n",
        "    z_vector = []\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        z_vector.append(z)\n",
        "        activation = self.activation_function(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "    return activations, z_vector\n",
        "\n",
        "\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "\n",
        "  def back_propagation(self, activations, z_vector, y):\n",
        "    b_error = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_error = [np.zeros(w.shape) for w in self.weights]\n",
        "    delta = self.cost_derivative(activations[-1], y) * self.activation_function_derivative(z_vector[-1])\n",
        "    b_error[-1] = delta\n",
        "    w_error[-1] = np.dot(delta, activations[-2].transpose())\n",
        "    for l in range(2, self.no_of_layers):\n",
        "        z = z_vector[-l]\n",
        "        sp = self.activation_function_derivative(z)\n",
        "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "        b_error[-l] = delta\n",
        "        w_error[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    return (b_error, w_error)\n",
        "\n",
        "\n",
        "  def fit(self, training_data, training_labels, epochs=20, batch_size = 10, learning_rate = 3.0):\n",
        "    n = len(training_data)\n",
        "    training_data_combined = list(zip(training_data, training_labels))\n",
        "    for iteration in range(epochs):\n",
        "      start_time = time.time()\n",
        "      self.update_weights_and_biases(training_data_combined, batch_size, learning_rate)\n",
        "      accuracy, loss = self.evaluate(training_data_combined)\n",
        "      duration = time.time() - start_time\n",
        "      print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for x, y in data:\n",
        "        output = self.forward_propagation(x)\n",
        "        predicted = np.argmax(output)\n",
        "        actual = np.argmax(y)\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "        total_loss += np.sum((output - y) ** 2)\n",
        "    accuracy = correct / len(data)\n",
        "    avg_loss = total_loss / len(data)\n",
        "    return accuracy, avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pw7FOzTq5FE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz -O mnist.pkl.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceijdXIYJ_i",
        "outputId": "d9c4c26a-ac38-4a97-9de5-ac8ab54c3e78"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-06 12:54:52--  https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz [following]\n",
            "--2025-04-06 12:54:53--  https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17051982 (16M) [application/octet-stream]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "\rmnist.pkl.gz          0%[                    ]       0  --.-KB/s               \rmnist.pkl.gz        100%[===================>]  16.26M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-04-06 12:54:53 (183 MB/s) - ‘mnist.pkl.gz’ saved [17051982/17051982]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBdZbdYnIaYp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "file_path = \"mnist.pkl.gz\"\n",
        "\n",
        "with gzip.open(file_path, 'rb') as f:\n",
        "    mnist_data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "training_data, validation_data, test_data = mnist_data\n",
        "print(training_data[0].shape)\n",
        "print(training_data[1].shape)\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "X_train = [x.reshape(-1, 1) for x in training_data[0]]\n",
        "y_train = [vectorized_label(y) for y in training_data[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDiZDPkY9z8",
        "outputId": "0255ac05-1165-4310-8059-7082826366c3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork([784,30,10])"
      ],
      "metadata": {
        "id": "5tmOoXc8IeNX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "gQGxRFfFIkHu",
        "outputId": "05c9e282-4e08-499b-bb27-66de183eb7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 10s  - accuracy: 0.9054 - loss: 0.1541\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 10s  - accuracy: 0.9245 - loss: 0.1258\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9336 - loss: 0.1101\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 10s  - accuracy: 0.9407 - loss: 0.1010\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9420 - loss: 0.0989\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 10s  - accuracy: 0.9472 - loss: 0.0920\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9497 - loss: 0.0853\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9500 - loss: 0.0867\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9549 - loss: 0.0793\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9575 - loss: 0.0734\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9568 - loss: 0.0742\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9562 - loss: 0.0759\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9581 - loss: 0.0727\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9602 - loss: 0.0682\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9625 - loss: 0.0657\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9605 - loss: 0.0679\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9626 - loss: 0.0668\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9647 - loss: 0.0624\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9641 - loss: 0.0623\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9660 - loss: 0.0605\n"
          ]
        }
      ]
    }
  ]
}