{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUUxzlIUHlwvW8gC3qNWh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datapirate09/Neural-Network-Builder/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "po-GlxcIpAYb"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "xqaLhvrsXytC"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, activation='sigmoid'):\n",
        "    #layers is a list of elements where each element shows no of neurons in that layer\n",
        "    self.no_of_layers = len(layers)\n",
        "    self.sizes = layers\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
        "    self.biases = [np.random.randn(y,1) for y in layers[1:]]\n",
        "    self.activation = activation\n",
        "\n",
        "\n",
        "  def get_shape_of_weights(self):\n",
        "    for i,layer in enumerate(self.weights):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def get_shape_of_bias(self):\n",
        "    for i,layer in enumerate(self.biases):\n",
        "      print(f\"Layer {i} has shape {layer.shape}\")\n",
        "\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "  def sigmoid_derivative(self, z):\n",
        "    return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "\n",
        "  def activation_function(self,activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid(activation_input)\n",
        "\n",
        "\n",
        "  def activation_function_derivative(self, activation_input):\n",
        "    if self.activation == 'sigmoid':\n",
        "      return self.sigmoid_derivative(activation_input)\n",
        "\n",
        "\n",
        "  def forward_propagation(self, a):\n",
        "    for i in range(self.no_of_layers-1):\n",
        "      a = self.activation_function(np.dot(self.weights[i],a) + self.biases[i])\n",
        "    return a\n",
        "\n",
        "\n",
        "  def update_weights_and_biases(self, input_data, batch_size=10, learning_rate=3.0):\n",
        "    random.shuffle(input_data)\n",
        "    mini_batches = [input_data[k:k+batch_size] for k in range(0, len(input_data), batch_size)]\n",
        "    for batch in mini_batches:\n",
        "        self.update_mini_batch(batch, learning_rate)\n",
        "\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, learning_rate=3.0):\n",
        "    b_diff = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_diff = [np.zeros(w.shape) for w in self.weights]\n",
        "    for data_item in mini_batch:\n",
        "        x, y = data_item  # training input and label\n",
        "        activations, z_vector = self.get_activations(x)\n",
        "        b_error, w_error = self.back_propagation(activations, z_vector, y)\n",
        "        b_diff = [bd + be for bd, be in zip(b_diff, b_error)]\n",
        "        w_diff = [wd + we for wd, we in zip(w_diff, w_error)]\n",
        "\n",
        "    self.weights = [w - (learning_rate / len(mini_batch)) * nw\n",
        "                    for w, nw in zip(self.weights, w_diff)]\n",
        "    self.biases = [b - (learning_rate / len(mini_batch)) * nb\n",
        "                   for b, nb in zip(self.biases, b_diff)]\n",
        "\n",
        "\n",
        "  def get_activations(self, input_data):\n",
        "    activation = input_data.reshape(-1, 1)  # Ensure it's a column vector\n",
        "    activations = [activation]\n",
        "    z_vector = []\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        z_vector.append(z)\n",
        "        activation = self.activation_function(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "    return activations, z_vector\n",
        "\n",
        "\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "\n",
        "  def back_propagation(self, activations, z_vector, y):\n",
        "    b_error = [np.zeros(b.shape) for b in self.biases]\n",
        "    w_error = [np.zeros(w.shape) for w in self.weights]\n",
        "    delta = self.cost_derivative(activations[-1], y) * self.activation_function_derivative(z_vector[-1])\n",
        "    b_error[-1] = delta\n",
        "    w_error[-1] = np.dot(delta, activations[-2].transpose())\n",
        "    for l in range(2, self.no_of_layers):\n",
        "        z = z_vector[-l]\n",
        "        sp = self.activation_function_derivative(z)\n",
        "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "        b_error[-l] = delta\n",
        "        w_error[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    return (b_error, w_error)\n",
        "\n",
        "\n",
        "  def fit(self, training_data, training_labels, epochs=20, batch_size = 10, learning_rate = 3.0):\n",
        "    n = len(training_data)\n",
        "    training_data_combined = list(zip(training_data, training_labels))\n",
        "    for iteration in range(epochs):\n",
        "      start_time = time.time()\n",
        "      self.update_weights_and_biases(training_data_combined, batch_size, learning_rate)\n",
        "      accuracy, loss = self.evaluate(training_data_combined)\n",
        "      duration = time.time() - start_time\n",
        "      print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for x, y in data:\n",
        "        output = self.forward_propagation(x)\n",
        "        predicted = np.argmax(output)\n",
        "        actual = np.argmax(y)\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "        total_loss += np.sum((output - y) ** 2)\n",
        "    accuracy = correct / len(data)\n",
        "    avg_loss = total_loss / len(data)\n",
        "    return accuracy, avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pw7FOzTq5FE"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz -O mnist.pkl.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceijdXIYJ_i",
        "outputId": "887731cf-3765-4242-dfc8-be9db88ee00b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-06 13:22:05--  https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz [following]\n",
            "--2025-04-06 13:22:06--  https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17051982 (16M) [application/octet-stream]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "mnist.pkl.gz        100%[===================>]  16.26M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-04-06 13:22:06 (187 MB/s) - ‘mnist.pkl.gz’ saved [17051982/17051982]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBdZbdYnIaYp"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "file_path = \"mnist.pkl.gz\"\n",
        "\n",
        "with gzip.open(file_path, 'rb') as f:\n",
        "    mnist_data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "training_data, validation_data, test_data = mnist_data\n",
        "print(training_data[0].shape)\n",
        "print(training_data[1].shape)\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "X_train = [x.reshape(-1, 1) for x in training_data[0]]\n",
        "y_train = [vectorized_label(y) for y in training_data[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDiZDPkY9z8",
        "outputId": "c4f84e2e-ae31-4745-b5ae-6634de79747d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork([784,30,10])"
      ],
      "metadata": {
        "id": "5tmOoXc8IeNX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQGxRFfFIkHu",
        "outputId": "d4d58761-0476-4de0-9505-790681cb21c7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.8993 - loss: 0.1642\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9297 - loss: 0.1172\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9316 - loss: 0.1143\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9402 - loss: 0.1010\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9496 - loss: 0.0861\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9476 - loss: 0.0871\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9503 - loss: 0.0847\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9538 - loss: 0.0799\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9541 - loss: 0.0781\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9568 - loss: 0.0745\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9571 - loss: 0.0733\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9573 - loss: 0.0726\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9625 - loss: 0.0634\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9621 - loss: 0.0657\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9639 - loss: 0.0632\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9644 - loss: 0.0616\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9657 - loss: 0.0592\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9658 - loss: 0.0581\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9654 - loss: 0.0590\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9668 - loss: 0.0568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = [x.reshape(-1, 1) for x in test_data[0]]\n",
        "y_test = [vectorized_label(y) for y in test_data[1]]\n",
        "testing_data_combined = list(zip(X_test, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)\n"
      ],
      "metadata": {
        "id": "lVA16rKLPukc",
        "outputId": "ab91fe7c-f322-4af8-8bf8-59e21a2f5896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9485\n",
            "0.08820091801539971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "(X_train_tf, y_train_tf), (X_test_tf, y_test_tf) = fashion_mnist.load_data()\n",
        "\n",
        "X_train_tf = [x.reshape(-1, 1) / 255.0 for x in X_train_tf]\n",
        "y_train_tf = [vectorized_label(y) for y in y_train_tf]\n",
        "\n",
        "X_test_tf = [x.reshape(-1, 1) / 255.0 for x in X_test_tf]\n",
        "y_test = [vectorized_label(y) for y in y_test_tf]\n",
        "\n",
        "model = NeuralNetwork([784, 64, 10])\n",
        "model.fit(X_train_tf, y_train_tf, epochs=10, batch_size=32, learning_rate=3.0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o6JwzlVgMUMN",
        "outputId": "4770eea8-58d9-4a16-c304-687c8ac97ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7493 - loss: 0.3547\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7744 - loss: 0.3132\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7856 - loss: 0.3009\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7882 - loss: 0.2916\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7971 - loss: 0.2780\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7972 - loss: 0.2803\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8072 - loss: 0.2646\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8101 - loss: 0.2587\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8092 - loss: 0.2620\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8123 - loss: 0.2603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data_combined = list(zip(X_test_tf, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)"
      ],
      "metadata": {
        "id": "-rCiJ3oHSHSd",
        "outputId": "721d3180-c9c9-4fa4-a52f-7d74f645ead2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7957\n",
            "0.28854674926965623\n"
          ]
        }
      ]
    }
  ]
}