{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXkLYPVC1H9aj/oGQYfTpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datapirate09/Neural-Network-Builder/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "po-GlxcIpAYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqaLhvrsXytC"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, activation='sigmoid'):\n",
        "    #layers is an array of objects where each object gives us the type of that layer\n",
        "    self.layer_info = layers\n",
        "    self.no_of_layers = len(layers)\n",
        "    self.sizes = [layer.no_of_neurons for layer in layers]\n",
        "    self.layer_activations = [layer.activation for layer in layers[1:]]\n",
        "    for i in range(1, self.no_of_layers):\n",
        "      self.layer_info[i].weights = np.random.randn(self.layer_info[i].no_of_neurons, self.layer_info[i-1].no_of_neurons)\n",
        "      self.layer_info[i].biases = np.random.randn(self.layer_info[i].no_of_neurons, 1)\n",
        "\n",
        "  def layers(self):\n",
        "    return self.layer_info\n",
        "\n",
        "  def summary(self):\n",
        "    total_params = 0\n",
        "    for i in range(1,self.no_of_layers):\n",
        "      layer_params_count = self.layer_info[i].weights.size + self.layer_info[i].biases.size\n",
        "      print(f\"L{i+1} params: {layer_params_count}\")\n",
        "      total_params += layer_params_count\n",
        "    print(f\"Total no of parameters:{total_params}\")\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "  def sigmoid_derivative(self, z):\n",
        "    return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "  def activation_function(self,activation_input, activation_fn_name='sigmoid'):\n",
        "    if activation_fn_name == 'sigmoid':\n",
        "      return self.sigmoid(activation_input)\n",
        "    elif activation_fn_name == 'softmax':\n",
        "      return self.softmax(activation_input)\n",
        "\n",
        "  def activation_function_derivative(self, activation_input, activation_fn_name='sigmoid'):\n",
        "    if activation_fn_name == 'sigmoid':\n",
        "      return self.sigmoid_derivative(activation_input)\n",
        "\n",
        "  def forward_propagation(self, a):\n",
        "    for i in range(self.no_of_layers-1):\n",
        "      a = self.activation_function(np.dot(self.layer_info[i+1].weights,a) + self.layer_info[i+1].biases, activation_fn_name = self.layer_activations[i])\n",
        "    return a\n",
        "\n",
        "  def update_weights_and_biases(self, input_data, batch_size=10, learning_rate=3.0):\n",
        "    random.shuffle(input_data)\n",
        "    mini_batches = [input_data[k:k+batch_size] for k in range(0, len(input_data), batch_size)]\n",
        "    for batch in mini_batches:\n",
        "        self.update_mini_batch(batch, learning_rate)\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, learning_rate=3.0):\n",
        "    b_diff = [np.zeros(b.biases.shape) for b in self.layer_info[1:]]\n",
        "    w_diff = [np.zeros(w.weights.shape) for w in self.layer_info[1:]]\n",
        "    for data_item in mini_batch:\n",
        "        x, y = data_item\n",
        "        activations, z_vector = self.get_activations(x)\n",
        "        b_error, w_error = self.back_propagation(activations, z_vector, y)\n",
        "        b_diff = [bd + be for bd, be in zip(b_diff, b_error)]\n",
        "        w_diff = [wd + we for wd, we in zip(w_diff, w_error)]\n",
        "    for i in range(1, self.no_of_layers):\n",
        "      self.layer_info[i].weights -= (learning_rate / len(mini_batch)) * w_diff[i-1]\n",
        "      self.layer_info[i].biases -= (learning_rate / len(mini_batch)) * b_diff[i-1]\n",
        "\n",
        "  def get_activations(self, input_data):\n",
        "    activation = input_data.reshape(-1, 1)\n",
        "    activations = [activation]\n",
        "    z_vector = []\n",
        "\n",
        "    for i in range(1,self.no_of_layers):\n",
        "      layer = self.layer_info[i]\n",
        "      z = np.dot(layer.weights, activation) + layer.biases\n",
        "      z_vector.append(z)\n",
        "      activation_fn_name = self.layer_activations[i-1]\n",
        "      activation = self.activation_function(z, activation_fn_name)\n",
        "      activations.append(activation)\n",
        "\n",
        "    return activations, z_vector\n",
        "\n",
        "  def softmax(self, z):\n",
        "    exp_z = np.exp(z - np.max(z))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "    return (output_activations-y)\n",
        "\n",
        "  def back_propagation(self, activations, z_vector, y):\n",
        "    b_error = [np.zeros(b.biases.shape) for b in self.layer_info[1:]]\n",
        "    w_error = [np.zeros(w.weights.shape) for w in self.layer_info[1:]]\n",
        "    delta = self.cost_derivative(activations[-1], y) * self.activation_function_derivative(z_vector[-1], self.layer_activations[-1])\n",
        "    # delta = activations[-1] - y\n",
        "    b_error[-1] = delta\n",
        "    w_error[-1] = np.dot(delta, activations[-2].transpose())\n",
        "    for l in range(2, self.no_of_layers):\n",
        "        z = z_vector[-l]\n",
        "        activation_name = self.layer_activations[-l]\n",
        "        sp = self.activation_function_derivative(z, activation_name)\n",
        "        delta = np.dot(self.layer_info[-l+1].weights.transpose(), delta) * sp\n",
        "        b_error[-l] = delta\n",
        "        w_error[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    return (b_error, w_error)\n",
        "\n",
        "  def fit(self, training_data, training_labels, validation_data=None, validation_labels=None, epochs=20, batch_size = 10, learning_rate = 3.0, callbacks=None):\n",
        "    n = len(training_data)\n",
        "    training_data_combined = list(zip(training_data, training_labels))\n",
        "    if validation_data != None and validation_labels != None:\n",
        "      validation_data_combined = list(zip(validation_data, validation_labels))\n",
        "    early_stopping = None\n",
        "    model_check_point = None\n",
        "    reduce_lr_on_plateau = None\n",
        "    if callbacks is None:\n",
        "      callbacks = []\n",
        "    for cb in callbacks:\n",
        "      if isinstance(cb, EarlyStopping):\n",
        "        early_stopping = cb\n",
        "      if isinstance(cb, ModelCheckPoint):\n",
        "        model_check_point = cb\n",
        "      if isinstance(cb, ReduceLROnPlateau):\n",
        "        reduce_lr_on_plateau = cb\n",
        "    for iteration in range(epochs):\n",
        "      start_time = time.time()\n",
        "      self.update_weights_and_biases(training_data_combined, batch_size, learning_rate)\n",
        "      accuracy, loss = self.evaluate(training_data_combined)\n",
        "      logs = {\"accuracy\": accuracy, \"loss\": loss}\n",
        "      duration = time.time() - start_time\n",
        "      if validation_data != None and validation_labels != None:\n",
        "        val_accuracy, val_loss = self.evaluate(validation_data_combined)\n",
        "        logs[\"val_accuracy\"] = val_accuracy\n",
        "        logs[\"val_loss\"] = val_loss\n",
        "        print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f} - val_accuracy: {val_accuracy:.4f} - val_loss: {val_loss:.4f}\")\n",
        "      else:\n",
        "        print(f\"{n}/{n} ━━━━━━━━━━━━━━━━━━━━ {duration:.0f}s  - accuracy: {accuracy:.4f} - loss: {loss:.4f}\")\n",
        "      if early_stopping:\n",
        "        early_stopping.on_epoch_end(iteration, logs)\n",
        "        if early_stopping.stop_training:\n",
        "            break\n",
        "      if model_check_point:\n",
        "        model_check_point.on_epoch_end(iteration, logs)\n",
        "        if model_check_point.show_check_point:\n",
        "          best_epoch_details = model_check_point.get_best_epoch_details()\n",
        "          print(f\"accuracy: {best_epoch_details['accuracy']:.4f} - loss: {best_epoch_details['loss']:.4f} - val_accuracy: {best_epoch_details['val_accuracy']:.4f} - val_loss: {best_epoch_details['val_loss']:.4f}\")\n",
        "      if reduce_lr_on_plateau:\n",
        "        factor = reduce_lr_on_plateau.on_epoch_end(iteration, logs)\n",
        "        if factor is not None:\n",
        "          new_lr = max(learning_rate * factor, reduce_lr_on_plateau.min_lr)\n",
        "          print(f\"Reducing learning rate from {learning_rate:.5f} to {new_lr:.5f}\")\n",
        "          learning_rate = new_lr\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for x, y in data:\n",
        "        output = self.forward_propagation(x)\n",
        "        predicted = np.argmax(output)\n",
        "        actual = np.argmax(y)\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "        total_loss += np.sum((output - y) ** 2)\n",
        "    accuracy = correct / len(data)\n",
        "    avg_loss = total_loss / len(data)\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "  def predict(self, data):\n",
        "    predictions = []\n",
        "    for item in data:\n",
        "      predictions.append(self.forward_propagation(item))\n",
        "    return predictions\n",
        "\n",
        "class EarlyStopping:\n",
        "  def __init__(self, monitor='val_loss', patience=0):\n",
        "    self.patience = patience\n",
        "    self.current_count = 0\n",
        "    self.monitor = monitor\n",
        "    self.best_value = None\n",
        "    self.stop_training = False\n",
        "\n",
        "  def on_epoch_end(self, epoch, epoch_info):\n",
        "    current = epoch_info.get(self.monitor)\n",
        "    if current is None:\n",
        "      return\n",
        "    if self.best_value == None or current < self.best_value:\n",
        "      self.best_value = current\n",
        "      self.current_count = 0\n",
        "    else:\n",
        "      self.current_count += 1\n",
        "      if self.current_count > self.patience:\n",
        "        self.stop_training = True\n",
        "\n",
        "\n",
        "class ModelCheckPoint:\n",
        "  def __init__(self, monitor='val_loss'):\n",
        "    self.monitor = monitor\n",
        "    self.show_check_point = False\n",
        "    self.previous_best = None\n",
        "    self.improved_epoch_details = []\n",
        "  def on_epoch_end(self, epoch, epoch_info):\n",
        "    current = epoch_info.get(self.monitor)\n",
        "    if self.previous_best == None or current < self.previous_best:\n",
        "      self.previous_best = current\n",
        "      self.store_epoch_details(epoch_info)\n",
        "      self.show_check_point = True\n",
        "    else:\n",
        "      self.show_check_point = False\n",
        "  def store_epoch_details(self, epoch_info):\n",
        "    self.improved_epoch_details.append(epoch_info)\n",
        "  def get_best_epoch_details(self):\n",
        "    return self.improved_epoch_details[-1]\n",
        "\n",
        "\n",
        "class ReduceLROnPlateau:\n",
        "  def __init__(self, monitor='val_loss', factor=0.1, patience=10, mode='min', min_lr=0.0):\n",
        "    self.monitor = monitor\n",
        "    self.factor = factor\n",
        "    self.patience = patience\n",
        "    self.mode = mode\n",
        "    self.min_lr = min_lr\n",
        "    self.current_count = 0\n",
        "    self.optimal_val = None\n",
        "\n",
        "  def on_epoch_end(self, epoch, epoch_info):\n",
        "    current = epoch_info.get(self.monitor)\n",
        "    if current is None:\n",
        "      return None\n",
        "    if self.optimal_val == None or self.check_optimal_condition(current):\n",
        "      self.optimal_val = current\n",
        "      self.current_count = 0\n",
        "      return None\n",
        "    else:\n",
        "      self.current_count += 1\n",
        "      if self.current_count > self.patience:\n",
        "        return self.factor\n",
        "\n",
        "  def check_optimal_condition(self, value):\n",
        "    if self.mode == 'min':\n",
        "      if value < self.optimal_val:\n",
        "        return True\n",
        "    elif self.mode == 'max':\n",
        "      if value > self.optimal_val:\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "  def __init__(self, no_of_neurons, activation='sigmoid'):\n",
        "    self.no_of_neurons = no_of_neurons\n",
        "    self.activation = activation\n",
        "    self.weights = None\n",
        "    self.biases = None\n",
        "\n",
        "  def get_weights(self):\n",
        "    return self.weights, self.biases\n"
      ],
      "metadata": {
        "id": "4pw7FOzTq5FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Input:\n",
        "  def __init__(self, no_of_neurons):\n",
        "    self.no_of_neurons = no_of_neurons"
      ],
      "metadata": {
        "id": "x-6nGbQZAJ6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "  def __init__(self, input_size, hidden_size, output_size, activation = 'tanh'):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.activation = activation\n",
        "    self.Wxh = np.random.randn(self.hidden_size, self.input_size)\n",
        "    self.Whh = np.random.randn(self.hidden_size, self.hidden_size)\n",
        "    self.Wyh = np.random.randn(self.output_size, self.hidden_size)\n",
        "    self.bh = np.random.randn(self.hidden_size, 1)\n",
        "    self.by = np.random.randn(self.output_size, 1)\n",
        "\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def tanh_derivative(h):\n",
        "    return 1 - h ** 2\n",
        "\n",
        "  def update_weights_and_biases(self, input_data, batch_size=10, learning_rate=3.0):\n",
        "    random.shuffle(input_data)\n",
        "    mini_batches = [input_data[k:k+batch_size] for k in range(0, len(input_data), batch_size)]\n",
        "\n",
        "    for batch in mini_batches:\n",
        "        self.update_mini_batch(batch, learning_rate)\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, learning_rate=3.0):\n",
        "    dWxh_total = np.zeros_like(self.Wxh)\n",
        "    dWhh_total = np.zeros_like(self.Whh)\n",
        "    dWyh_total = np.zeros_like(self.Wyh)\n",
        "    dbh_total = np.zeros_like(self.bh)\n",
        "    dby_total = np.zeros_like(self.by)\n",
        "\n",
        "    for data_item in mini_batch:\n",
        "        x, y = data_item\n",
        "        ys, hs = self.forward_prop(x)\n",
        "        grads = self.backward_prop(x, y, hs, ys)\n",
        "        dWxh_total += grads['dWxh']\n",
        "        dWhh_total += grads['dWhh']\n",
        "        dWyh_total += grads['dWyh']\n",
        "        dbh_total += grads['dbh']\n",
        "        dby_total += grads['dby']\n",
        "\n",
        "    self.Wxh -= learning_rate * dWxh_total / len(mini_batch)\n",
        "    self.Whh -= learning_rate * dWhh_total / len(mini_batch)\n",
        "    self.Wyh -= learning_rate * dWyh_total / len(mini_batch)\n",
        "    self.bh -= learning_rate * dbh_total / len(mini_batch)\n",
        "    self.by -= learning_rate * dby_total / len(mini_batch)\n",
        "\n",
        "\n",
        "  def fit(self, sequences, targets, epochs=20, batch_size=10, learning_rate=3.0):\n",
        "    for epoch in range(epochs):\n",
        "      self.update_weights_and_biases(list(zip(sequences, targets)), batch_size, learning_rate)\n",
        "      print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n",
        "\n",
        "\n",
        "  def forward_prop(self, sequence):\n",
        "    h = np.zeros((self.hidden_size, 1))\n",
        "    hs = [h]\n",
        "    ys = []\n",
        "\n",
        "    for x_t in sequence:\n",
        "        x_t = x_t.reshape(self.input_size, 1)\n",
        "        a = np.dot(self.Wxh, x_t) + np.dot(self.Whh, h) + self.bh\n",
        "        h = self.tanh(a)\n",
        "        y = np.dot(self.Wyh, h) + self.by\n",
        "        hs.append(h)\n",
        "        ys.append(y)\n",
        "    return ys, hs[1:]\n",
        "\n",
        "  def backward_prop(self, sequence, targets, hs, ys):\n",
        "    dWxh = np.zeros_like(self.Wxh)\n",
        "    dWhh = np.zeros_like(self.Whh)\n",
        "    dWyh = np.zeros_like(self.Wyh)\n",
        "    dbh = np.zeros_like(self.bh)\n",
        "    dby = np.zeros_like(self.by)\n",
        "\n",
        "    h0 = np.zeros_like(hs[0])\n",
        "    hs_full = [h0] + hs\n",
        "    dh_next = np.zeros_like(hs[0])\n",
        "\n",
        "    for t in reversed(range(len(sequence))):\n",
        "        x_t = sequence[t].reshape(self.input_size, 1)\n",
        "        y_t = targets[t].reshape(self.output_size, 1)\n",
        "        h_t = hs_full[t + 1]\n",
        "        h_prev = hs_full[t]\n",
        "        y_pred = ys[t]\n",
        "\n",
        "        dy = y_pred - y_t\n",
        "        dWyh += np.dot(dy, h_t.T)\n",
        "        dby += dy\n",
        "\n",
        "        dh = np.dot(self.Wyh.T, dy) + dh_next\n",
        "        da = dh * self.tanh_derivative(h_t)\n",
        "\n",
        "        dWxh += np.dot(da, x_t.T)\n",
        "        dWhh += np.dot(da, h_prev.T)\n",
        "        dbh += da\n",
        "\n",
        "        dh_next = np.dot(self.Whh.T, da)\n",
        "\n",
        "    return {\n",
        "        'dWxh': dWxh,\n",
        "        'dWhh': dWhh,\n",
        "        'dWyh': dWyh,\n",
        "        'dbh': dbh,\n",
        "        'dby': dby\n",
        "    }\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9Ec-oTPPd381"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCHgkNwFlp0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "class Tokenizer:\n",
        "  def __init__(self, num_words = 100, lower=True):\n",
        "    self.num_words = num_words\n",
        "    self.lower = lower\n",
        "    self.word_count = defaultdict(int)\n",
        "    self.word_index = {}\n",
        "  def fit_on_texts(self, inputs):\n",
        "    for input in inputs:\n",
        "      input = input.split()\n",
        "      if self.lower:\n",
        "        input = [x.lower() for x in input]\n",
        "      for word in input:\n",
        "        self.word_count[word] += 1\n",
        "    sorted_words = sorted(self.word_count.items(), key=lambda x: -x[1])\n",
        "    for i,word in enumerate(sorted_words[:self.num_words]):\n",
        "      self.word_index[word[0]] = i+1\n",
        "  def texts_to_sequences(self, input):\n",
        "    sequence = []\n",
        "    input = input.split()\n",
        "    for word in input:\n",
        "      sequence.append(self.word_index.get(word,0))\n",
        "    return sequence\n",
        "  def pad_sequences(self, seq, truncating='post', padding='post', maxlen=100):\n",
        "    if len(seq) > maxlen:\n",
        "        if truncating == 'post':\n",
        "            seq = seq[:maxlen]\n",
        "        else:\n",
        "            seq = seq[-maxlen:]\n",
        "    elif len(seq) < maxlen:\n",
        "        pad_length = maxlen - len(seq)\n",
        "        if padding == 'post':\n",
        "            seq = seq + [0] * pad_length\n",
        "        else:\n",
        "            seq = [0] * pad_length + seq\n",
        "    return seq\n",
        "\n"
      ],
      "metadata": {
        "id": "aRozc8TOW2nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nlp\n",
        "# !pip install datasets\n",
        "\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset('emotion')\n",
        "# print(dataset.shape)\n",
        "# dataset\n",
        "# train = dataset['train']\n",
        "# validation = dataset['validation']\n",
        "# test = dataset['test']\n",
        "# train_tweets = [tweet['text'] for tweet in train]\n",
        "# train_labels = [tweet['label'] for tweet in train]\n",
        "# print(train_tweets[0])\n",
        "# print(train_labels[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lKo0o-xxijh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_tweets)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# print(\"Word Index:\", word_index)\n",
        "print(\"First Tweet:\", train_tweets[0])\n",
        "tokenized_seq = tokenizer.texts_to_sequences(train_tweets[0])\n",
        "print(\"Tokenized Sequence:\", tokenized_seq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCQtIyKdi2h9",
        "outputId": "f9e4f756-a8f4-48f6-f94a-13831040ad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Tweet: i didnt feel humiliated\n",
            "Tokenized Sequence: [1, 138, 2, 678]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 50\n",
        "sequences = tokenizer.texts_to_sequences(train_tweets[0])\n",
        "padded = tokenizer.pad_sequences(sequences, truncating='post', padding='post', maxlen=maxlen)\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8rsHjAHli84",
        "outputId": "3b0c1412-3981-4a9e-b977-a675f4e937ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 138, 2, 678, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz -O mnist.pkl.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceijdXIYJ_i",
        "outputId": "7256bfdc-f6e3-49c2-b09e-bcd7bf79f1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-09 15:20:08--  https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz [following]\n",
            "--2025-04-09 15:20:09--  https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17051982 (16M) [application/octet-stream]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "mnist.pkl.gz        100%[===================>]  16.26M  33.3MB/s    in 0.5s    \n",
            "\n",
            "2025-04-09 15:20:10 (33.3 MB/s) - ‘mnist.pkl.gz’ saved [17051982/17051982]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBdZbdYnIaYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "file_path = \"mnist.pkl.gz\"\n",
        "\n",
        "with gzip.open(file_path, 'rb') as f:\n",
        "    mnist_data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "training_data, validation_data, test_data = mnist_data\n",
        "print(training_data[0].shape)\n",
        "print(training_data[1].shape)\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "X_train = [x.reshape(-1, 1) for x in training_data[0]]\n",
        "y_train = [vectorized_label(y) for y in training_data[1]]\n",
        "\n",
        "X_val = [x.reshape(-1, 1) for x in validation_data[0]]\n",
        "y_val = [vectorized_label(y) for y in validation_data[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDiZDPkY9z8",
        "outputId": "6edf1595-255d-42e2-93db-a764819e3043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = Input(784)\n",
        "layer2 = Dense(30, activation='sigmoid')\n",
        "layer3 = Dense(10,activation='sigmoid')\n",
        "model = NeuralNetwork([layer1, layer2, layer3])\n",
        "model.summary()\n",
        "layers = model.layers()\n",
        "W1,b1 = layers[1].get_weights()\n",
        "W2,b2 = layers[2].get_weights()\n",
        "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
        "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")"
      ],
      "metadata": {
        "id": "5tmOoXc8IeNX",
        "outputId": "b3d84bec-21d6-4c3f-844e-38086447934a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 params: 23550\n",
            "L3 params: 310\n",
            "Total no of parameters:23860\n",
            "W1 shape = (30, 784), b1 shape = (30, 1)\n",
            "W2 shape = (10, 30), b2 shape = (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_check_point = ModelCheckPoint(monitor='val_loss')\n",
        "model.fit(X_train, y_train, X_val, y_val, callbacks=[early_stopping,model_check_point])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQGxRFfFIkHu",
        "outputId": "d9acbaa3-6f29-4386-e166-064275614643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9087 - loss: 0.1493 - val_accuracy: 0.9176 - val_loss: 0.1367\n",
            "accuracy: 0.9087 - loss: 0.1493 - val_accuracy: 0.9176 - val_loss: 0.1367\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9245 - loss: 0.1252 - val_accuracy: 0.9239 - val_loss: 0.1244\n",
            "accuracy: 0.9245 - loss: 0.1252 - val_accuracy: 0.9239 - val_loss: 0.1244\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9371 - loss: 0.1054 - val_accuracy: 0.9363 - val_loss: 0.1045\n",
            "accuracy: 0.9371 - loss: 0.1054 - val_accuracy: 0.9363 - val_loss: 0.1045\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9393 - loss: 0.1003 - val_accuracy: 0.9365 - val_loss: 0.1033\n",
            "accuracy: 0.9393 - loss: 0.1003 - val_accuracy: 0.9365 - val_loss: 0.1033\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9455 - loss: 0.0919 - val_accuracy: 0.9428 - val_loss: 0.0949\n",
            "accuracy: 0.9455 - loss: 0.0919 - val_accuracy: 0.9428 - val_loss: 0.0949\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9420 - loss: 0.0955 - val_accuracy: 0.9412 - val_loss: 0.1010\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9497 - loss: 0.0850 - val_accuracy: 0.9459 - val_loss: 0.0921\n",
            "accuracy: 0.9497 - loss: 0.0850 - val_accuracy: 0.9459 - val_loss: 0.0921\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9530 - loss: 0.0815 - val_accuracy: 0.9483 - val_loss: 0.0904\n",
            "accuracy: 0.9530 - loss: 0.0815 - val_accuracy: 0.9483 - val_loss: 0.0904\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9538 - loss: 0.0792 - val_accuracy: 0.9473 - val_loss: 0.0898\n",
            "accuracy: 0.9538 - loss: 0.0792 - val_accuracy: 0.9473 - val_loss: 0.0898\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9572 - loss: 0.0747 - val_accuracy: 0.9491 - val_loss: 0.0890\n",
            "accuracy: 0.9572 - loss: 0.0747 - val_accuracy: 0.9491 - val_loss: 0.0890\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9572 - loss: 0.0750 - val_accuracy: 0.9476 - val_loss: 0.0891\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 12s  - accuracy: 0.9603 - loss: 0.0685 - val_accuracy: 0.9496 - val_loss: 0.0852\n",
            "accuracy: 0.9603 - loss: 0.0685 - val_accuracy: 0.9496 - val_loss: 0.0852\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9598 - loss: 0.0699 - val_accuracy: 0.9487 - val_loss: 0.0875\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 8s  - accuracy: 0.9565 - loss: 0.0750 - val_accuracy: 0.9454 - val_loss: 0.0946\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9611 - loss: 0.0665 - val_accuracy: 0.9485 - val_loss: 0.0884\n",
            "50000/50000 ━━━━━━━━━━━━━━━━━━━━ 9s  - accuracy: 0.9584 - loss: 0.0717 - val_accuracy: 0.9473 - val_loss: 0.0901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = [x.reshape(-1, 1) for x in test_data[0]]\n",
        "y_test = [vectorized_label(y) for y in test_data[1]]\n",
        "testing_data_combined = list(zip(X_test, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions[0])\n"
      ],
      "metadata": {
        "id": "lVA16rKLPukc",
        "outputId": "256913f3-61a8-4d00-9214-fa372c6015b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9441\n",
            "0.09584845820981042\n",
            "[[5.90443612e-06]\n",
            " [5.66600827e-04]\n",
            " [5.70669822e-05]\n",
            " [9.34571025e-04]\n",
            " [1.01822058e-07]\n",
            " [6.74290551e-06]\n",
            " [4.64898698e-11]\n",
            " [9.99465235e-01]\n",
            " [1.13471439e-08]\n",
            " [2.94557186e-08]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "def vectorized_label(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "(X_train_tf, y_train_tf), (X_test_tf, y_test_tf) = fashion_mnist.load_data()\n",
        "\n",
        "X_train_tf = [x.reshape(-1, 1) / 255.0 for x in X_train_tf]\n",
        "y_train_tf = [vectorized_label(y) for y in y_train_tf]\n",
        "\n",
        "X_test_tf = [x.reshape(-1, 1) / 255.0 for x in X_test_tf]\n",
        "y_test = [vectorized_label(y) for y in y_test_tf]\n",
        "\n",
        "model = NeuralNetwork([784, 64, 10])\n",
        "model.fit(X_train_tf, y_train_tf, epochs=10, batch_size=32, learning_rate=3.0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o6JwzlVgMUMN",
        "outputId": "4770eea8-58d9-4a16-c304-687c8ac97ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7493 - loss: 0.3547\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7744 - loss: 0.3132\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7856 - loss: 0.3009\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7882 - loss: 0.2916\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7971 - loss: 0.2780\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.7972 - loss: 0.2803\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8072 - loss: 0.2646\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8101 - loss: 0.2587\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8092 - loss: 0.2620\n",
            "60000/60000 ━━━━━━━━━━━━━━━━━━━━ 17s  - accuracy: 0.8123 - loss: 0.2603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data_combined = list(zip(X_test_tf, y_test))\n",
        "\n",
        "accuracy, avg_loss = model.evaluate(testing_data_combined)\n",
        "print(accuracy)\n",
        "print(avg_loss)"
      ],
      "metadata": {
        "id": "-rCiJ3oHSHSd",
        "outputId": "721d3180-c9c9-4fa4-a52f-7d74f645ead2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7957\n",
            "0.28854674926965623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpVhN-9MpdB5"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}